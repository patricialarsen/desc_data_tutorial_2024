{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35799286-1fa3-40ef-81d9-49afe47c79fe",
   "metadata": {},
   "source": [
    "# Data Access Tutorial\n",
    "\n",
    "Let's get started setting up your environment. First let's clone this github directory somewhere on your perlmutter home directory so you can follow along, then if you haven't recently updated your desc kernels go ahead and do this. This will prevent old software issues during the week.\n",
    "\n",
    "\n",
    "<div markdown=\"1\" style=\"background-color:rgba(230, 230, 250, 0.8); text-align:left; vertical-align: middle; padding:10px; width:900px; border:solid;\">\n",
    "<p style='margin-left:1em;'>\n",
    "    \n",
    "* Log in to jupyter notebooks on NERSC \n",
    "     > https://jupyter.nersc.gov/\n",
    "* Git clone this repo (You can select File&#8594;New&#8594;Terminal to open up a terminal window, and in your home directory type)\n",
    "     > git clone https://github.com/patricialarsen/desc_data_tutorial_2024.git\n",
    "* Set up your kernels (in the same terminal window type the following)\n",
    "    > source /global/common/software/lsst/common/miniconda/kernels/setup.sh\n",
    "* Open tutorial in a notebook with the desc-python kernel\n",
    "</p></span>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f2ab5a-a670-4b76-9a0b-6e5a58fd2238",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<div style=\"background-color:rgba(200, 230, 250, 0.4); text-align:left; vertical-align: middle; padding:10px; width:900px;\">\n",
    "\n",
    "<p>\n",
    "\n",
    "\n",
    "### Background: GCR Catalogs/ The Data Registry /Parquet\n",
    "\n",
    "[GCRCatalogs](https://github.com/LSSTDESC/gcr-catalogs) is a DESC project which aims at gathering in one convenient location various simulation/data catalogs made available to the collaboration. This was designed to make it easy to read in many types of data (simulations and precursor data from various groups) which all have different formats and units, and convert them to a unified interface. This has been very useful for simulated data validation when catalog formats are constantly evolving. \n",
    "\n",
    "For LSST data releases the formats and schema of the data are for the most part now set. This means that the GCR is simply using parquet to filter and return the data as a dictionary - I'm using it here as a shortcut at times, but will also show you how to do the parquet read. To do a direct read you need the file location and metadata, in future the data registry will provide this but as it's currently in a testing phase I'll show you how to access this through the GCR.  \n",
    "\n",
    "\n",
    "</p></span>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cb46b9-9cde-43d5-9b0c-c75fabcc7e88",
   "metadata": {},
   "source": [
    "As always we need to start with some basic preamble. The only new thing here should be to add the system path for the SRV version of the GCR catalogs (the DP0.2 reader will be merged in with the main one eventually but for now it's available here) before we read in the catalog reader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37264ce-6ac4-4067-bed2-5632e35446e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "sys.path.insert(0,'/global/cfs/projectdirs/lsst/groups/SRV/gcr-catalogs-test')\n",
    "import GCRCatalogs\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a671ef2d-42c6-4b68-9c80-b4f4ced10781",
   "metadata": {},
   "source": [
    "A significant numbers of cosmology catalogs besides the DP0.2 object catalogs are also available through the GCR, use `sorted(GCRCatalogs.get_available_catalogs(False))` to see a full list and visit the [DC2 Data Product](https://confluence.slac.stanford.edu/display/LSSTDESC/DC2+Data+Product+Overview) page to see all the DC2-related catalogs. We're going to use the DP0.2 object catalogs, so let's look for those. I've called the catalog we're looking for lsst_object. Let's look for possible cadidates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52971b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for catalog in sorted(GCRCatalogs.get_available_catalogs(False)):\n",
    "    if 'lsst' in catalog:\n",
    "        print(catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717511e5-5b91-4d76-ac87-492339c60f1c",
   "metadata": {},
   "source": [
    "You'll see some SSI catalogs here (synthetic source injection catalogs), as well as the full object catalog and a smaller version of the object catalog (only 1 tract of data). For now let's use lsst_object as the full dataset is a little large for everyone to read in at the same time! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f4720-c75d-4788-bd96-0b0023315f04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "catalog = GCRCatalogs.load_catalog('lsst_object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eed31e-692b-4ce3-9942-da2d5429ae19",
   "metadata": {},
   "source": [
    "### DP0.2 object catalog Schema\n",
    "\n",
    "The quantities available in the catalog can be found on the project website here: https://sdm-schemas.lsst.io/dp02.html, but we can also access these from the GCR in the following way. Note that these methods are equivalent, for this specific catalog named lsst_object we have done no quantity translation and so it's safe to assume that these follow the lsst conventions exactly. \n",
    "\n",
    "<div markdown=\"1\" style=\"background-color:rgba(200, 230, 250, 0.4); text-align:middle; vertical-align: middle; padding:10px; width:900px;\">\n",
    "<p style='margin-left:100em;'>\n",
    "\n",
    "### GCR Note\n",
    "\n",
    "For those of you familiar with the GCR, you would normally look at list_all_quantities in a catalog with quantity modifiers. However the lsst reader used here is designed to exactly read in the Parquet data with no modifiers, so you instead look at list_all_native_quantities to get those directly. An example of non-native quantities is if we define modifiers to do useful things like convert fluxes to magnitudes, or define complex filter sets. \n",
    "\n",
    "Note that for comparisons between the DP0.2 and DC2 object catalogs you may prefer to use a translation layer to convert the DP0.2 catalog into DC2 conventions before read-in, this is possible by swapping to the dp02_object_single_tract catalog rather than lsst_object, and using the quantities list rather than the native_quantities list. Many validation tests are currently designed to work with this translation layer to enable such comparisons.\n",
    "\n",
    "</p></span>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d2c0c-0869-4ab1-a1a4-737f26764c6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quantity_list = sorted(catalog.list_all_native_quantities())\n",
    "for i in range(4):\n",
    "    q = quantity_list[i]\n",
    "    info = catalog.get_quantity_info(q)\n",
    "    print(q)\n",
    "    print('------------------')\n",
    "    print(info['description'])\n",
    "    print('Type: ' + info['type'] + ',', 'Units: ' + info['unit'])\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41803b69-6f58-419f-9801-ddd0ef255595",
   "metadata": {},
   "source": [
    "## Accessing the dataset through the GCR\n",
    "\n",
    "Typically in the GCR we list the quantities we require, filters we want to cut these on, and any native filters we want to cut on. Native filters define the set of files to include within the group so tend to result in a more efficient filter.  \n",
    "\n",
    "### Notation and format\n",
    "- Filters are defined using strings (e.g. filters = ['r_ra > 0', 'r_decl < 0']) , these are converted to parquet formatting through the reader \n",
    "- specifying return_iterator=True will return the data iteratively, for this catalog we combine the files into a pyarrow dataset and iterative outputs are computed using the default to_batches method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339dd3df-b8c5-40c3-9d7f-fd31b7ad93a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = catalog.get_quantities([\"r_ra\",\"r_decl\"])  \n",
    "plt.figure()\n",
    "plt.hist2d(data[\"r_ra\"],data[\"r_decl\"],bins=500)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0dee6-f8a0-4cfc-8328-e59ee947227f",
   "metadata": {},
   "source": [
    "### Hmm, that looks weird..\n",
    "\n",
    "Luckily for us I've added a handy panic button to the notebook (click the button!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0929473-1d7e-4967-bd5b-48eb5899dc50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "button = widgets.Button(description=\"Panic time\", layout=widgets.Layout(width='10%', height='40px') )\n",
    "button.style.button_color = 'lightblue'\n",
    "output = widgets.Output()\n",
    "\n",
    "def panic_button_clicked(b):\n",
    "    with output:\n",
    "        image = Image.open('./pictures/panic.png')\n",
    "        display(image.resize((400,400),2))\n",
    "\n",
    "button.on_click(panic_button_clicked)\n",
    "widgets.Box([button, output])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161b40e-e8f0-4776-b5f4-ad1f2eed50c7",
   "metadata": {},
   "source": [
    "### What happened??\n",
    "\n",
    "The lesson here is to be careful what you wish for! If you want to directly read the data we need to take the time to understand it a little better before jumping in. What happened here is that duplicate objects measured at the boundaries between tracts and patches haven't been removed from the catalog, resulting in a bright grid hiding all our stars and galaxies. \n",
    "\n",
    "If you want to inspect a normal looking galaxy catalog, you need to apply filters to remove duplicate objects, false sky objects and stars from the catalog. I've got a draft description of the base LSST flags and filters that we would recommend always applying here:\n",
    "https://lsstdesc.org/srv-dp02/tutorials/Rubin/tutorial7_SRV_releases.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895da10f-7f14-4c3d-90a6-72c773aef3ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filters = ['detect_isPrimary','refExtendedness==1']\n",
    "data = catalog.get_quantities([\"r_ra\",\"r_decl\"], filters =filters)  \n",
    "\n",
    "plt.figure()\n",
    "plt.hist2d(data[\"r_ra\"],data[\"r_decl\"],bins=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b1cbe5-efc2-49bb-9173-77c2e67d74bd",
   "metadata": {},
   "source": [
    "### Time to update that button.. \n",
    "\n",
    "<img src=\"./pictures/dontpanic.png\"  width=\"340\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ecb6c6-ecba-4dc7-9355-c073985033cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "button = widgets.Button(description=\"Resolve Panic\", layout=widgets.Layout(width='10%', height='40px') )\n",
    "button.style.button_color = 'lightblue'\n",
    "output = widgets.Output()\n",
    "\n",
    "def depanic_button_clicked(b):\n",
    "    with output:\n",
    "        image = Image.open('./pictures/dontpanic.png')\n",
    "        display(image.resize((400,400),2))\n",
    "\n",
    "button.on_click(depanic_button_clicked)\n",
    "widgets.Box([button, output])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46f4e9e-e49f-429d-8104-5c7dee5afcbf",
   "metadata": {},
   "source": [
    "## Okay okay I get it, now what if I want to read the files directly?\n",
    "\n",
    "Just remember a few things: \n",
    "* You'll need to know the locations (can do this either through the data registry - currently in beta phase, or through the GCR)\n",
    "* There is no quantity translation so if two catalogs have different schema you'll have to resolve these\n",
    "* Remember to take care of any necessary cuts, filters and catalog-level blinding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59211f00-335f-4724-ac0c-a5bbe1d79157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyarrow.compute as pc\n",
    "import pyarrow.dataset as ds\n",
    "from GCR.utils import concatenate_1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b71c8f-408e-462c-8de1-b25c055d949c",
   "metadata": {},
   "source": [
    "In future we'd advise you to use the data registry, but for now you can access the datasets directly from the GCR catalog. Let's find the file corresponding to this (single-tract) catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db12aa0-59b7-4b3f-988d-ba4754716000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for parquet_data in catalog._datasets:\n",
    "    print('Dataset info: ', parquet_data.info)\n",
    "    print('Dataset handle: ', parquet_data.handle)\n",
    "    print('Dataset path: ', parquet_data.path)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586d90c2-ce22-4699-aeed-d450f3a94ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take the file handles and combine them into a dataset (you can also provide the path if you want all files in the folder, \n",
    "# or in this case we can use a single file)\n",
    "sources = [parquet_data.path for parquet_data in catalog._datasets]\n",
    "dataset = ds.dataset(sources)\n",
    "\n",
    "# the filter format is a little different, pc is pyarrow.compute\n",
    "filter = pc.field('detect_isPrimary')& (pc.field(\"refExtendedness\") == 1) \n",
    "\n",
    "# and let's try this with a batched read\n",
    "ra_full = []\n",
    "dec_full = []\n",
    "for batch in dataset.to_batches(columns=[\"r_ra\",\"r_decl\"], filter=filter,):\n",
    "    ra_full.append(batch[\"r_ra\"])\n",
    "    dec_full.append(batch[\"r_decl\"])\n",
    "time_end = time.time()\n",
    "\n",
    "ra_full = concatenate_1d(ra_full)\n",
    "dec_full = concatenate_1d(dec_full)\n",
    "\n",
    "plt.hist2d(ra_full,dec_full,bins=1000)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d33d9-b0e0-4d37-82de-236de32ddfe6",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div markdown=\"1\" style=\"background-color:rgba(200, 230, 250, 0.4); text-align:middle; vertical-align: middle; padding:10px; width:900px;\">\n",
    "<p style='margin-left:100em;'>\n",
    "\n",
    "### I'm not familiar with parquet filters..  \n",
    "\n",
    "Do I really need to convert all of these from the string format?\n",
    "\n",
    "Lucky for you we convert these within the GCR already, so you can just import that function \n",
    "> from GCRCatalogs.parquet import convert_to_pyarrow_filter \n",
    "\n",
    "This can be useful if you're provided with a long list or are just unfamiliar with parquet filters. The full code is here:\n",
    "\n",
    "```python\n",
    "import re\n",
    "import numexpr as ne\n",
    "\n",
    "def convert_to_pyarrow_filter(filter_list):\n",
    "    for i in range(len(filter_list)):\n",
    "        for n in set(ne.necompiler.precompile(filter_list[i])[-1]):\n",
    "            filter_list[i] = re.sub(rf\"\\b({n})\\b\", \"pc.field(\\\"\\\\1\\\")\", filter_list[i])\n",
    "    total_query = eval(filter_list[0])\n",
    "    \n",
    "    if len(filter_list)>1:\n",
    "        for i in range(1,len(filter_list)):\n",
    "            total_query = total_query&eval(filter_list[i])\n",
    "    return total_query\n",
    "```\n",
    "\n",
    "</p></span>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e1c50-24a2-458c-aef4-e62ce7421e39",
   "metadata": {},
   "source": [
    "## Adding more filters\n",
    "\n",
    "Let's try that again with the same functions as before, but this time let's refine the data a little better. As a shortcut I'm going to get a list of flags which I should require to be False from the GCR quantity modifiers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32f6a1a-8128-48cf-b3ac-a7dd51be7ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "catalog.get_quantity_modifier('flags_bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fa610a-c67c-4ba1-b5b5-76d15e26c67f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from GCRCatalogs.parquet import convert_to_pyarrow_filter \n",
    "\n",
    "# take the file handles and combine them into a dataset\n",
    "sources = [parquet_data.path for parquet_data in catalog._datasets]\n",
    "dataset = ds.dataset(sources)\n",
    "\n",
    "filter_list = [flag + '==False' for flag in catalog.get_quantity_modifier('flags_bad')[1:]]\n",
    "filter_list += ['detect_isPrimary', 'refExtendedness == 1', \"r_ra > 0\", \"r_decl < -40\"]\n",
    "filter_list_pyarrow = convert_to_pyarrow_filter(filter_list)\n",
    "\n",
    "data = dataset.to_table(columns=[\"r_ra\",\"r_decl\"], filter=filter,)\n",
    "\n",
    "plt.hist2d(data['r_ra'],data['r_decl'],bins=1000)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a5ba7d-0a40-4bbc-b99d-98c161619d90",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task! Get some data\n",
    "\n",
    "Now we have some nice looking galaxy data!\n",
    "\n",
    "In the next section we'll be creating a validation test on the PSF quantities. So let's started by reading in some data! With the filters detect_isPrimary, and refExtendedness ==1 (galaxies only), get the following quantities in the g band:\n",
    "\n",
    "* From the DP0.2 schema look for the PSF shapes (use PSF rather than DebiasedPSF)\n",
    "* Compute the trace of the PSF (ixx + iyy)\n",
    "* Look for cModel magnitudes, and read in the g band magnitudes (you'll need to convert from fluxes)\n",
    "* Get RA/ DEC\n",
    "\n",
    "Hints:\n",
    "* Check utils.py for a flux to magnitude conversion function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desc-python",
   "language": "python",
   "name": "desc-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
